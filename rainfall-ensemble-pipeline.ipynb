{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåßÔ∏è Rainfall Prediction - Advanced Ensemble Pipeline\n",
    "\n",
    "## Competition: Playground Series S5E3 - Binary Prediction with a Rainfall Dataset\n",
    "\n",
    "### Strategy Overview\n",
    "This notebook implements a **state-of-the-art ensemble approach** targeting top 10-20% performance:\n",
    "\n",
    "1. **Feature Engineering**: 100+ features from 10 original features\n",
    "   - Temperature, pressure, wind, humidity features\n",
    "   - **Lag features** (critical for temporal weather data)\n",
    "   - Interaction features and statistical aggregates\n",
    "\n",
    "2. **8 Diverse Base Models**:\n",
    "   - XGBoost, LightGBM, CatBoost (gradient boosting)\n",
    "   - Random Forest, Extra Trees (bagging)\n",
    "   - Logistic Regression (linear baseline)\n",
    "   - Model variants for diversity\n",
    "\n",
    "3. **Two-Level Stacking Ensemble**:\n",
    "   - 10-fold stratified CV with consistent folds\n",
    "   - Out-of-fold predictions for stacking\n",
    "   - Meta-learner + final blending\n",
    "\n",
    "**Target**: AUC 0.90+ (baseline ~0.87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Gradient Boosting\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Utilities\n",
    "from scipy import stats\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    # Reproducibility\n",
    "    SEED = 42\n",
    "    N_FOLDS = 10\n",
    "    \n",
    "    # Column names\n",
    "    TARGET = 'rainfall'\n",
    "    ID_COL = 'id'\n",
    "    DAY_COL = 'day'\n",
    "    \n",
    "    # Original numeric features\n",
    "    ORIGINAL_FEATURES = [\n",
    "        'pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint',\n",
    "        'humidity', 'cloud', 'sunshine', 'winddirection', 'windspeed'\n",
    "    ]\n",
    "    \n",
    "    # Features to use for lag calculations\n",
    "    LAG_FEATURES = ['pressure', 'temparature', 'humidity', 'cloud', 'windspeed', 'dewpoint']\n",
    "    \n",
    "    @staticmethod\n",
    "    def seed_everything(seed=SEED):\n",
    "        \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set seed\n",
    "Config.seed_everything()\n",
    "print(f\"üé≤ Random seed set to {Config.SEED}\")\n",
    "print(f\"üìä Using {Config.N_FOLDS}-fold cross-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "# Kaggle paths\n",
    "KAGGLE_INPUT = '/kaggle/input/playground-series-s5e3'\n",
    "LOCAL_INPUT = './data'\n",
    "\n",
    "# Try Kaggle path first, fallback to local\n",
    "if os.path.exists(KAGGLE_INPUT):\n",
    "    INPUT_PATH = KAGGLE_INPUT\n",
    "    print(\"üìÇ Running on Kaggle\")\n",
    "else:\n",
    "    INPUT_PATH = LOCAL_INPUT\n",
    "    print(\"üìÇ Running locally\")\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(f'{INPUT_PATH}/train.csv')\n",
    "test = pd.read_csv(f'{INPUT_PATH}/test.csv')\n",
    "\n",
    "print(f\"\\nüìä Training data shape: {train.shape}\")\n",
    "print(f\"üìä Test data shape: {test.shape}\")\n",
    "print(f\"\\nüéØ Target distribution:\")\n",
    "print(train[Config.TARGET].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPLORATORY DATA ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìã Data Types:\")\n",
    "print(train.dtypes)\n",
    "\n",
    "print(\"\\nüìã Missing Values:\")\n",
    "print(train.isnull().sum())\n",
    "\n",
    "print(\"\\nüìã Basic Statistics:\")\n",
    "display(train.describe())\n",
    "\n",
    "print(\"\\nüìã First 5 rows:\")\n",
    "display(train.head())\n",
    "\n",
    "print(\"\\nüéØ Target Distribution:\")\n",
    "print(f\"  No Rain (0): {(train[Config.TARGET] == 0).sum()} ({(train[Config.TARGET] == 0).mean()*100:.1f}%)\")\n",
    "print(f\"  Rain (1): {(train[Config.TARGET] == 1).sum()} ({(train[Config.TARGET] == 1).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Feature Engineering\n",
    "\n",
    "Creating **100+ features** from 10 original features across these categories:\n",
    "- Temperature features (15+)\n",
    "- Pressure features (12+)\n",
    "- Wind features (15+)\n",
    "- Cloud/Sunshine features (12+)\n",
    "- Humidity features (10+)\n",
    "- **Lag/Temporal features (20+)** - Critical for weather prediction\n",
    "- Interaction features (20+)\n",
    "- Statistical features (10+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEMPERATURE FEATURES\n",
    "# ============================================================================\n",
    "def create_temperature_features(df):\n",
    "    \"\"\"Create temperature-based features (~15 features).\"\"\"\n",
    "    \n",
    "    # Basic temperature features\n",
    "    df['temp_range'] = df['maxtemp'] - df['mintemp']\n",
    "    df['temp_mean'] = (df['maxtemp'] + df['mintemp']) / 2\n",
    "    df['temp_amplitude'] = df['maxtemp'] - df['temp_mean']\n",
    "    \n",
    "    # Temperature vs average\n",
    "    df['temp_skew'] = (df['temparature'] - df['temp_mean']) / (df['temp_range'] + 1e-6)\n",
    "    df['temp_position'] = (df['temparature'] - df['mintemp']) / (df['temp_range'] + 1e-6)\n",
    "    \n",
    "    # Dewpoint relationships (moisture indicators)\n",
    "    df['temp_dew_diff'] = df['temparature'] - df['dewpoint']  # Dew point depression\n",
    "    df['temp_dew_ratio'] = df['temparature'] / (df['dewpoint'] + 273.15)  # Kelvin ratio\n",
    "    df['saturation_deficit'] = df['temp_dew_diff']  # Same as dew point depression\n",
    "    \n",
    "    # Relative humidity calculation (Magnus formula approximation)\n",
    "    df['rh_calculated'] = 100 * np.exp(\n",
    "        (17.625 * df['dewpoint']) / (243.04 + df['dewpoint']) -\n",
    "        (17.625 * df['temparature']) / (243.04 + df['temparature'])\n",
    "    )\n",
    "    \n",
    "    # Temperature stability\n",
    "    df['temp_stability'] = df['temp_range'] / (df['temp_mean'] + 1e-6)\n",
    "    df['temp_variability'] = df['temp_range'] / (df['temparature'] + 1e-6)\n",
    "    \n",
    "    # Polynomial features\n",
    "    df['temp_squared'] = df['temparature'] ** 2\n",
    "    df['temp_cubed'] = df['temparature'] ** 3\n",
    "    df['dewpoint_squared'] = df['dewpoint'] ** 2\n",
    "    df['temp_range_squared'] = df['temp_range'] ** 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Temperature features function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRESSURE FEATURES\n",
    "# ============================================================================\n",
    "def create_pressure_features(df):\n",
    "    \"\"\"Create pressure-based features (~12 features).\"\"\"\n",
    "    \n",
    "    # Pressure change (requires sorted data)\n",
    "    df['pressure_change'] = df['pressure'].diff().fillna(0)\n",
    "    df['pressure_change_abs'] = df['pressure_change'].abs()\n",
    "    \n",
    "    # Rate of change\n",
    "    df['pressure_change_rate'] = df['pressure_change'] / (df['pressure'].shift(1) + 1e-6)\n",
    "    df['pressure_change_rate'] = df['pressure_change_rate'].fillna(0)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    df['pressure_roll_mean_3'] = df['pressure'].rolling(3, min_periods=1).mean()\n",
    "    df['pressure_roll_std_3'] = df['pressure'].rolling(3, min_periods=1).std().fillna(0)\n",
    "    df['pressure_roll_mean_7'] = df['pressure'].rolling(7, min_periods=1).mean()\n",
    "    df['pressure_roll_std_7'] = df['pressure'].rolling(7, min_periods=1).std().fillna(0)\n",
    "    \n",
    "    # Deviation from rolling mean\n",
    "    df['pressure_dev_3'] = df['pressure'] - df['pressure_roll_mean_3']\n",
    "    df['pressure_dev_7'] = df['pressure'] - df['pressure_roll_mean_7']\n",
    "    \n",
    "    # Pressure categories (meteorological significance)\n",
    "    df['low_pressure'] = (df['pressure'] < 1013).astype(np.int8)  # Below standard atmosphere\n",
    "    df['high_pressure'] = (df['pressure'] > 1020).astype(np.int8)\n",
    "    df['pressure_falling'] = (df['pressure_change'] < -2).astype(np.int8)\n",
    "    df['pressure_rising'] = (df['pressure_change'] > 2).astype(np.int8)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Pressure features function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WIND FEATURES\n",
    "# ============================================================================\n",
    "def create_wind_features(df):\n",
    "    \"\"\"Create wind-based features (~15 features).\"\"\"\n",
    "    \n",
    "    # Wind vector components\n",
    "    wind_rad = np.radians(df['winddirection'])\n",
    "    df['wind_x'] = df['windspeed'] * np.cos(wind_rad)  # East-West component\n",
    "    df['wind_y'] = df['windspeed'] * np.sin(wind_rad)  # North-South component\n",
    "    \n",
    "    # Wind direction categories (8 cardinal directions)\n",
    "    df['wind_dir_N'] = ((df['winddirection'] >= 337.5) | (df['winddirection'] < 22.5)).astype(np.int8)\n",
    "    df['wind_dir_NE'] = ((df['winddirection'] >= 22.5) & (df['winddirection'] < 67.5)).astype(np.int8)\n",
    "    df['wind_dir_E'] = ((df['winddirection'] >= 67.5) & (df['winddirection'] < 112.5)).astype(np.int8)\n",
    "    df['wind_dir_SE'] = ((df['winddirection'] >= 112.5) & (df['winddirection'] < 157.5)).astype(np.int8)\n",
    "    df['wind_dir_S'] = ((df['winddirection'] >= 157.5) & (df['winddirection'] < 202.5)).astype(np.int8)\n",
    "    df['wind_dir_SW'] = ((df['winddirection'] >= 202.5) & (df['winddirection'] < 247.5)).astype(np.int8)\n",
    "    df['wind_dir_W'] = ((df['winddirection'] >= 247.5) & (df['winddirection'] < 292.5)).astype(np.int8)\n",
    "    df['wind_dir_NW'] = ((df['winddirection'] >= 292.5) & (df['winddirection'] < 337.5)).astype(np.int8)\n",
    "    \n",
    "    # Wind strength categories\n",
    "    df['wind_calm'] = (df['windspeed'] < 5).astype(np.int8)\n",
    "    df['wind_moderate'] = ((df['windspeed'] >= 5) & (df['windspeed'] < 15)).astype(np.int8)\n",
    "    df['wind_strong'] = (df['windspeed'] >= 15).astype(np.int8)\n",
    "    \n",
    "    # Wind chill factor (simplified formula)\n",
    "    # Only valid for temp < 10¬∞C and wind > 4.8 km/h, but we apply generally\n",
    "    df['wind_chill'] = np.where(\n",
    "        df['windspeed'] > 0,\n",
    "        13.12 + 0.6215 * df['temparature'] - 11.37 * (df['windspeed'] ** 0.16) + \n",
    "        0.3965 * df['temparature'] * (df['windspeed'] ** 0.16),\n",
    "        df['temparature']\n",
    "    )\n",
    "    \n",
    "    # Wind kinetic energy proxy\n",
    "    df['wind_energy'] = 0.5 * df['windspeed'] ** 2\n",
    "    \n",
    "    # Wind speed polynomial\n",
    "    df['windspeed_squared'] = df['windspeed'] ** 2\n",
    "    df['windspeed_log'] = np.log1p(df['windspeed'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Wind features function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLOUD & SUNSHINE FEATURES\n",
    "# ============================================================================\n",
    "def create_cloud_sunshine_features(df):\n",
    "    \"\"\"Create cloud and sunshine features (~12 features).\"\"\"\n",
    "    \n",
    "    # Sunshine transformations\n",
    "    df['sunshine_log'] = np.log1p(df['sunshine'])\n",
    "    df['sunshine_sqrt'] = np.sqrt(df['sunshine'])\n",
    "    df['sunshine_squared'] = df['sunshine'] ** 2\n",
    "    \n",
    "    # Sun fraction (sunshine vs total sky)\n",
    "    total_sky = df['cloud'] + df['sunshine'] + 1e-6\n",
    "    df['sun_frac'] = df['sunshine'] / total_sky\n",
    "    df['cloud_frac'] = df['cloud'] / total_sky\n",
    "    \n",
    "    # Cloud categories\n",
    "    df['overcast'] = (df['cloud'] >= 80).astype(np.int8)\n",
    "    df['mostly_cloudy'] = ((df['cloud'] >= 50) & (df['cloud'] < 80)).astype(np.int8)\n",
    "    df['partly_cloudy'] = ((df['cloud'] >= 20) & (df['cloud'] < 50)).astype(np.int8)\n",
    "    df['mostly_clear'] = (df['cloud'] < 20).astype(np.int8)\n",
    "    \n",
    "    # Sunshine duration categories\n",
    "    df['no_sun'] = (df['sunshine'] == 0).astype(np.int8)\n",
    "    df['some_sun'] = ((df['sunshine'] > 0) & (df['sunshine'] < 5)).astype(np.int8)\n",
    "    df['sunny'] = (df['sunshine'] >= 5).astype(np.int8)\n",
    "    \n",
    "    # Cloud polynomial\n",
    "    df['cloud_squared'] = df['cloud'] ** 2\n",
    "    df['cloud_log'] = np.log1p(df['cloud'])\n",
    "    \n",
    "    # Cloud-sunshine interaction\n",
    "    df['cloud_sun_product'] = df['cloud'] * df['sunshine']\n",
    "    df['cloud_sun_diff'] = df['cloud'] - df['sunshine']\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Cloud/Sunshine features function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HUMIDITY FEATURES\n",
    "# ============================================================================\n",
    "def create_humidity_features(df):\n",
    "    \"\"\"Create humidity-based features (~10 features).\"\"\"\n",
    "    \n",
    "    # Humidity transformations\n",
    "    df['humidity_squared'] = df['humidity'] ** 2\n",
    "    df['humidity_log'] = np.log1p(df['humidity'])\n",
    "    df['humidity_sqrt'] = np.sqrt(df['humidity'])\n",
    "    \n",
    "    # Humidity categories\n",
    "    df['very_humid'] = (df['humidity'] >= 90).astype(np.int8)\n",
    "    df['humid'] = ((df['humidity'] >= 70) & (df['humidity'] < 90)).astype(np.int8)\n",
    "    df['moderate_humidity'] = ((df['humidity'] >= 40) & (df['humidity'] < 70)).astype(np.int8)\n",
    "    df['dry'] = (df['humidity'] < 40).astype(np.int8)\n",
    "    \n",
    "    # Absolute humidity approximation (grams of water per cubic meter)\n",
    "    # Using simplified Magnus formula\n",
    "    df['absolute_humidity'] = (\n",
    "        6.112 * np.exp((17.67 * df['temparature']) / (df['temparature'] + 243.5)) *\n",
    "        df['humidity'] * 2.1674 / (273.15 + df['temparature'])\n",
    "    )\n",
    "    \n",
    "    # Vapor pressure (hPa)\n",
    "    df['vapor_pressure'] = 6.112 * np.exp(\n",
    "        (17.67 * df['dewpoint']) / (df['dewpoint'] + 243.5)\n",
    "    )\n",
    "    \n",
    "    # Saturation vapor pressure\n",
    "    df['saturation_vapor_pressure'] = 6.112 * np.exp(\n",
    "        (17.67 * df['temparature']) / (df['temparature'] + 243.5)\n",
    "    )\n",
    "    \n",
    "    # Vapor pressure deficit\n",
    "    df['vapor_pressure_deficit'] = df['saturation_vapor_pressure'] - df['vapor_pressure']\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Humidity features function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LAG / TEMPORAL FEATURES (CRITICAL!)\n",
    "# ============================================================================\n",
    "def create_lag_features(df, is_train=True):\n",
    "    \"\"\"\n",
    "    Create lag and temporal features (~25 features).\n",
    "    \n",
    "    CRITICAL: Weather patterns are temporal - previous day conditions\n",
    "    strongly predict current day rainfall.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort by day for proper temporal ordering\n",
    "    df = df.sort_values('day').reset_index(drop=True)\n",
    "    \n",
    "    lag_cols = Config.LAG_FEATURES\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        # Lag features (1-3 days back)\n",
    "        df[f'{col}_lag1'] = df[col].shift(1)\n",
    "        df[f'{col}_lag2'] = df[col].shift(2)\n",
    "        df[f'{col}_lag3'] = df[col].shift(3)\n",
    "        \n",
    "        # Differences (change from previous days)\n",
    "        df[f'{col}_diff1'] = df[col] - df[col].shift(1)\n",
    "        df[f'{col}_diff2'] = df[col] - df[col].shift(2)\n",
    "        \n",
    "        # Rolling statistics (3-day and 7-day windows)\n",
    "        df[f'{col}_roll_mean_3'] = df[col].rolling(3, min_periods=1).mean()\n",
    "        df[f'{col}_roll_std_3'] = df[col].rolling(3, min_periods=1).std().fillna(0)\n",
    "        df[f'{col}_roll_mean_7'] = df[col].rolling(7, min_periods=1).mean()\n",
    "        \n",
    "        # Deviation from rolling mean\n",
    "        df[f'{col}_roll_dev_3'] = df[col] - df[f'{col}_roll_mean_3']\n",
    "    \n",
    "    # Fill NaN values created by lag/shift operations\n",
    "    # Use forward fill then backward fill\n",
    "    df = df.fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    # Also fill any remaining NaN with column median\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Lag features function defined (CRITICAL for weather prediction!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INTERACTION FEATURES\n",
    "# ============================================================================\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"Create interaction features between key variables (~20 features).\"\"\"\n",
    "    \n",
    "    # Key feature interactions (multiplicative)\n",
    "    df['pressure_x_humidity'] = df['pressure'] * df['humidity']\n",
    "    df['pressure_x_temp'] = df['pressure'] * df['temparature']\n",
    "    df['pressure_x_cloud'] = df['pressure'] * df['cloud']\n",
    "    df['temp_x_humidity'] = df['temparature'] * df['humidity']\n",
    "    df['temp_x_cloud'] = df['temparature'] * df['cloud']\n",
    "    df['temp_x_windspeed'] = df['temparature'] * df['windspeed']\n",
    "    df['humidity_x_cloud'] = df['humidity'] * df['cloud']\n",
    "    df['humidity_x_windspeed'] = df['humidity'] * df['windspeed']\n",
    "    df['cloud_x_windspeed'] = df['cloud'] * df['windspeed']\n",
    "    df['dewpoint_x_humidity'] = df['dewpoint'] * df['humidity']\n",
    "    \n",
    "    # Ratio interactions\n",
    "    df['pressure_humidity_ratio'] = df['pressure'] / (df['humidity'] + 1)\n",
    "    df['temp_windspeed_ratio'] = df['temparature'] / (df['windspeed'] + 1)\n",
    "    df['cloud_humidity_ratio'] = df['cloud'] / (df['humidity'] + 1)\n",
    "    df['humidity_dewpoint_ratio'] = df['humidity'] / (df['dewpoint'] + 30)  # Shift to avoid negative\n",
    "    df['pressure_temp_ratio'] = df['pressure'] / (df['temparature'] + 273.15)  # Kelvin\n",
    "    \n",
    "    # Complex meteorological indices\n",
    "    \n",
    "    # Heat Index (simplified Rothfusz regression)\n",
    "    df['heat_index'] = (\n",
    "        -42.379 + 2.04901523 * df['temparature'] + 10.14333127 * df['humidity'] -\n",
    "        0.22475541 * df['temparature'] * df['humidity'] -\n",
    "        6.83783e-3 * df['temparature']**2 - 5.481717e-2 * df['humidity']**2 +\n",
    "        1.22874e-3 * df['temparature']**2 * df['humidity'] +\n",
    "        8.5282e-4 * df['temparature'] * df['humidity']**2 -\n",
    "        1.99e-6 * df['temparature']**2 * df['humidity']**2\n",
    "    )\n",
    "    \n",
    "    # Lifted Condensation Level proxy (meters)\n",
    "    # LCL ‚âà 125 * (T - Td)\n",
    "    df['lcl_proxy'] = 125 * (df['temparature'] - df['dewpoint'])\n",
    "    \n",
    "    # K-Index (thunderstorm potential)\n",
    "    # Simplified: K = T850 - T500 + Td850 - (T700 - Td700)\n",
    "    # We approximate with surface values\n",
    "    df['k_index_proxy'] = df['temparature'] + df['dewpoint'] - df['temp_dew_diff']\n",
    "    \n",
    "    # Precipitation likelihood index (custom)\n",
    "    df['precip_index'] = (\n",
    "        df['humidity'] / 100 * \n",
    "        df['cloud'] / 100 * \n",
    "        (100 - df['temp_dew_diff']) / 100\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Interaction features function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STATISTICAL FEATURES\n",
    "# ============================================================================\n",
    "def create_statistical_features(df, train_stats=None):\n",
    "    \"\"\"\n",
    "    Create statistical features (~12 features).\n",
    "    Uses training statistics for both train and test to prevent leakage.\n",
    "    \"\"\"\n",
    "    \n",
    "    numeric_cols = Config.ORIGINAL_FEATURES\n",
    "    \n",
    "    if train_stats is None:\n",
    "        # Calculate stats from this dataframe (training)\n",
    "        train_stats = {\n",
    "            'mean': df[numeric_cols].mean(),\n",
    "            'std': df[numeric_cols].std(),\n",
    "            'median': df[numeric_cols].median(),\n",
    "            'min': df[numeric_cols].min(),\n",
    "            'max': df[numeric_cols].max()\n",
    "        }\n",
    "    \n",
    "    # Z-score normalization (using training stats)\n",
    "    for col in numeric_cols:\n",
    "        df[f'{col}_zscore'] = (df[col] - train_stats['mean'][col]) / (train_stats['std'][col] + 1e-6)\n",
    "    \n",
    "    # Row-level aggregates\n",
    "    df['row_mean'] = df[numeric_cols].mean(axis=1)\n",
    "    df['row_std'] = df[numeric_cols].std(axis=1)\n",
    "    df['row_min'] = df[numeric_cols].min(axis=1)\n",
    "    df['row_max'] = df[numeric_cols].max(axis=1)\n",
    "    df['row_range'] = df['row_max'] - df['row_min']\n",
    "    df['row_skew'] = df[numeric_cols].skew(axis=1)\n",
    "    \n",
    "    return df, train_stats\n",
    "\n",
    "print(\"‚úÖ Statistical features function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MASTER FEATURE ENGINEERING FUNCTION\n",
    "# ============================================================================\n",
    "def create_all_features(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Apply all feature engineering to train and test data.\n",
    "    Returns feature-engineered dataframes.\n",
    "    \"\"\"\n",
    "    print(\"üîß Starting feature engineering...\")\n",
    "    print(f\"   Initial train shape: {train_df.shape}\")\n",
    "    print(f\"   Initial test shape: {test_df.shape}\")\n",
    "    \n",
    "    # Make copies\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "    \n",
    "    # Store IDs and target\n",
    "    train_ids = train[Config.ID_COL].values\n",
    "    test_ids = test[Config.ID_COL].values\n",
    "    y = train[Config.TARGET].values\n",
    "    \n",
    "    # Combine for consistent feature engineering\n",
    "    train['is_train'] = 1\n",
    "    test['is_train'] = 0\n",
    "    if Config.TARGET in test.columns:\n",
    "        test = test.drop(Config.TARGET, axis=1)\n",
    "    \n",
    "    combined = pd.concat([train.drop(Config.TARGET, axis=1), test], axis=0, ignore_index=True)\n",
    "    print(f\"   Combined shape: {combined.shape}\")\n",
    "    \n",
    "    # Apply feature engineering (order matters for lag features)\n",
    "    print(\"   Creating temperature features...\")\n",
    "    combined = create_temperature_features(combined)\n",
    "    \n",
    "    print(\"   Creating pressure features...\")\n",
    "    combined = create_pressure_features(combined)\n",
    "    \n",
    "    print(\"   Creating wind features...\")\n",
    "    combined = create_wind_features(combined)\n",
    "    \n",
    "    print(\"   Creating cloud/sunshine features...\")\n",
    "    combined = create_cloud_sunshine_features(combined)\n",
    "    \n",
    "    print(\"   Creating humidity features...\")\n",
    "    combined = create_humidity_features(combined)\n",
    "    \n",
    "    print(\"   Creating lag/temporal features (CRITICAL)...\")\n",
    "    combined = create_lag_features(combined)\n",
    "    \n",
    "    print(\"   Creating interaction features...\")\n",
    "    combined = create_interaction_features(combined)\n",
    "    \n",
    "    print(\"   Creating statistical features...\")\n",
    "    # Calculate stats on training data only\n",
    "    train_mask = combined['is_train'] == 1\n",
    "    _, train_stats = create_statistical_features(combined[train_mask].copy())\n",
    "    combined, _ = create_statistical_features(combined, train_stats)\n",
    "    \n",
    "    # Split back\n",
    "    train_fe = combined[combined['is_train'] == 1].drop('is_train', axis=1).reset_index(drop=True)\n",
    "    test_fe = combined[combined['is_train'] == 0].drop('is_train', axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Restore IDs\n",
    "    train_fe[Config.ID_COL] = train_ids\n",
    "    test_fe[Config.ID_COL] = test_ids\n",
    "    \n",
    "    print(f\"\\n‚úÖ Feature engineering complete!\")\n",
    "    print(f\"   Final train shape: {train_fe.shape}\")\n",
    "    print(f\"   Final test shape: {test_fe.shape}\")\n",
    "    print(f\"   Total features: {train_fe.shape[1] - 2}\")\n",
    "    \n",
    "    return train_fe, test_fe, y\n",
    "\n",
    "print(\"‚úÖ Master feature engineering function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# APPLY FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "train_fe, test_fe, y = create_all_features(train, test)\n",
    "\n",
    "# Get feature columns (exclude id and day)\n",
    "feature_cols = [c for c in train_fe.columns if c not in [Config.ID_COL, Config.DAY_COL]]\n",
    "\n",
    "print(f\"\\nüìä Feature Summary:\")\n",
    "print(f\"   Total features: {len(feature_cols)}\")\n",
    "print(f\"   Training samples: {len(train_fe)}\")\n",
    "print(f\"   Test samples: {len(test_fe)}\")\n",
    "\n",
    "# Prepare final feature matrices\n",
    "X = train_fe[feature_cols].values\n",
    "X_test = test_fe[feature_cols].values\n",
    "\n",
    "print(f\"\\n‚úÖ Feature matrices ready:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Cross-Validation Setup\n",
    "\n",
    "**Critical**: Use the exact same fold indices across ALL models for proper stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CROSS-VALIDATION SETUP\n",
    "# ============================================================================\n",
    "def create_cv_folds(y, n_splits=Config.N_FOLDS, seed=Config.SEED):\n",
    "    \"\"\"\n",
    "    Create stratified k-fold indices.\n",
    "    CRITICAL: These exact folds must be used for ALL models.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    fold_indices = list(skf.split(np.zeros(len(y)), y))\n",
    "    \n",
    "    print(f\"‚úÖ Created {n_splits}-fold stratified CV\")\n",
    "    for i, (train_idx, val_idx) in enumerate(fold_indices):\n",
    "        train_pos = y[train_idx].sum() / len(train_idx) * 100\n",
    "        val_pos = y[val_idx].sum() / len(val_idx) * 100\n",
    "        print(f\"   Fold {i+1}: Train={len(train_idx)} ({train_pos:.1f}% pos), Val={len(val_idx)} ({val_pos:.1f}% pos)\")\n",
    "    \n",
    "    return fold_indices\n",
    "\n",
    "# Create fold indices (used by ALL models)\n",
    "FOLD_INDICES = create_cv_folds(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OUT-OF-FOLD PREDICTION GENERATOR\n",
    "# ============================================================================\n",
    "def generate_oof_predictions(model_name, model_fn, X, y, X_test, fold_indices, \n",
    "                            use_early_stopping=False, eval_metric='auc'):\n",
    "    \"\"\"\n",
    "    Train model with CV and generate out-of-fold predictions.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name for logging\n",
    "        model_fn: Function that returns a new model instance\n",
    "        X: Training features (numpy array)\n",
    "        y: Training labels (numpy array)\n",
    "        X_test: Test features (numpy array)\n",
    "        fold_indices: List of (train_idx, val_idx) tuples\n",
    "        use_early_stopping: Whether to use early stopping (for boosting models)\n",
    "        eval_metric: Metric for early stopping\n",
    "    \n",
    "    Returns:\n",
    "        oof_preds: Out-of-fold predictions for training data\n",
    "        test_preds: Averaged predictions for test data\n",
    "        cv_scores: List of fold AUC scores\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    n_folds = len(fold_indices)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(fold_indices):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Get fresh model instance\n",
    "        model = model_fn()\n",
    "        \n",
    "        # Train\n",
    "        if use_early_stopping:\n",
    "            if 'XGB' in model_name or 'xgb' in str(type(model)):\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    verbose=False\n",
    "                )\n",
    "            elif 'LGB' in model_name or 'lgb' in str(type(model)):\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)]\n",
    "                )\n",
    "            elif 'Cat' in model_name or 'catboost' in str(type(model)):\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=(X_val, y_val),\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        val_pred = model.predict_proba(X_val)[:, 1]\n",
    "        test_pred = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Store predictions\n",
    "        oof_preds[val_idx] = val_pred\n",
    "        test_preds += test_pred / n_folds\n",
    "        \n",
    "        # Calculate fold score\n",
    "        fold_auc = roc_auc_score(y_val, val_pred)\n",
    "        cv_scores.append(fold_auc)\n",
    "        print(f\"   Fold {fold_idx + 1}: AUC = {fold_auc:.5f}\")\n",
    "    \n",
    "    # Calculate overall CV score\n",
    "    cv_mean = np.mean(cv_scores)\n",
    "    cv_std = np.std(cv_scores)\n",
    "    oof_auc = roc_auc_score(y, oof_preds)\n",
    "    \n",
    "    print(f\"\\n   CV Mean AUC: {cv_mean:.5f} (+/- {cv_std:.5f})\")\n",
    "    print(f\"   OOF AUC: {oof_auc:.5f}\")\n",
    "    \n",
    "    return oof_preds, test_preds, cv_scores\n",
    "\n",
    "print(\"‚úÖ OOF prediction generator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ Model Definitions\n",
    "\n",
    "Defining 8 diverse base models for the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 1: XGBOOST\n",
    "# ============================================================================\n",
    "def get_xgb_model():\n",
    "    return xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='auc',\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        n_estimators=1000,\n",
    "        early_stopping_rounds=100,\n",
    "        random_state=Config.SEED,\n",
    "        tree_method='hist',\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ XGBoost model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 2: LIGHTGBM\n",
    "# ============================================================================\n",
    "def get_lgb_model():\n",
    "    return lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric='auc',\n",
    "        learning_rate=0.01,\n",
    "        max_depth=8,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        n_estimators=1000,\n",
    "        random_state=Config.SEED,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ LightGBM model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 3: CATBOOST\n",
    "# ============================================================================\n",
    "def get_cat_model():\n",
    "    return CatBoostClassifier(\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='AUC',\n",
    "        learning_rate=0.03,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        iterations=1000,\n",
    "        early_stopping_rounds=100,\n",
    "        random_seed=Config.SEED,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ CatBoost model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 4: RANDOM FOREST\n",
    "# ============================================================================\n",
    "def get_rf_model():\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=12,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1,\n",
    "        random_state=Config.SEED\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Random Forest model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 5: EXTRA TREES\n",
    "# ============================================================================\n",
    "def get_et_model():\n",
    "    return ExtraTreesClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=12,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1,\n",
    "        random_state=Config.SEED\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Extra Trees model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 6: LOGISTIC REGRESSION (with scaling)\n",
    "# ============================================================================\n",
    "class ScaledLogisticRegression:\n",
    "    \"\"\"Logistic Regression with built-in feature scaling.\"\"\"\n",
    "    \n",
    "    def __init__(self, C=0.1, random_state=Config.SEED):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = LogisticRegression(\n",
    "            C=C,\n",
    "            penalty='l2',\n",
    "            solver='lbfgs',\n",
    "            max_iter=1000,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict_proba(X_scaled)\n",
    "\n",
    "def get_lr_model():\n",
    "    return ScaledLogisticRegression(C=0.1, random_state=Config.SEED)\n",
    "\n",
    "print(\"‚úÖ Logistic Regression model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 7: XGBOOST VARIANT (different hyperparameters for diversity)\n",
    "# ============================================================================\n",
    "def get_xgb_v2_model():\n",
    "    return xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='auc',\n",
    "        learning_rate=0.02,\n",
    "        max_depth=4,  # Shallower trees\n",
    "        min_child_weight=5,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=2.0,\n",
    "        n_estimators=1000,\n",
    "        early_stopping_rounds=100,\n",
    "        random_state=123,  # Different seed\n",
    "        tree_method='hist',\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ XGBoost v2 model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 8: LIGHTGBM VARIANT (different hyperparameters for diversity)\n",
    "# ============================================================================\n",
    "def get_lgb_v2_model():\n",
    "    return lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric='auc',\n",
    "        learning_rate=0.02,\n",
    "        max_depth=6,\n",
    "        num_leaves=63,  # More leaves\n",
    "        min_child_samples=30,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=2.0,\n",
    "        n_estimators=1000,\n",
    "        random_state=456,  # Different seed\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ LightGBM v2 model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Train All Base Models\n",
    "\n",
    "Training all 8 models and collecting out-of-fold predictions for stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN ALL BASE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Dictionary to store results\n",
    "oof_predictions = {}\n",
    "test_predictions = {}\n",
    "cv_scores_all = {}\n",
    "\n",
    "# Define all models\n",
    "models = {\n",
    "    'XGBoost': (get_xgb_model, True),\n",
    "    'LightGBM': (get_lgb_model, True),\n",
    "    'CatBoost': (get_cat_model, True),\n",
    "    'RandomForest': (get_rf_model, False),\n",
    "    'ExtraTrees': (get_et_model, False),\n",
    "    'LogisticReg': (get_lr_model, False),\n",
    "    'XGBoost_v2': (get_xgb_v2_model, True),\n",
    "    'LightGBM_v2': (get_lgb_v2_model, True),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ALL BASE MODELS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Models to train: {list(models.keys())}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Folds: {len(FOLD_INDICES)}\")\n",
    "\n",
    "# Train each model\n",
    "for model_name, (model_fn, use_early_stopping) in models.items():\n",
    "    oof, test_pred, scores = generate_oof_predictions(\n",
    "        model_name=model_name,\n",
    "        model_fn=model_fn,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        X_test=X_test,\n",
    "        fold_indices=FOLD_INDICES,\n",
    "        use_early_stopping=use_early_stopping\n",
    "    )\n",
    "    \n",
    "    oof_predictions[model_name] = oof\n",
    "    test_predictions[model_name] = test_pred\n",
    "    cv_scores_all[model_name] = scores\n",
    "    \n",
    "    # Clean up memory\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASE MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARIZE BASE MODEL RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\nüìä BASE MODEL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(cv_scores_all.keys()),\n",
    "    'CV Mean AUC': [np.mean(scores) for scores in cv_scores_all.values()],\n",
    "    'CV Std': [np.std(scores) for scores in cv_scores_all.values()],\n",
    "    'OOF AUC': [roc_auc_score(y, oof_predictions[name]) for name in cv_scores_all.keys()]\n",
    "}).sort_values('OOF AUC', ascending=False)\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "print(f\"\\nüèÜ Best single model: {results_df.iloc[0]['Model']} (AUC: {results_df.iloc[0]['OOF AUC']:.5f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Stacking Ensemble\n",
    "\n",
    "Building a two-level stacking ensemble using out-of-fold predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BUILD STACKING FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILDING STACKING ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Stack OOF predictions as features\n",
    "model_names = list(oof_predictions.keys())\n",
    "n_models = len(model_names)\n",
    "\n",
    "# Create stacking feature matrix\n",
    "X_stack = np.column_stack([oof_predictions[name] for name in model_names])\n",
    "X_test_stack = np.column_stack([test_predictions[name] for name in model_names])\n",
    "\n",
    "print(f\"\\nStacking features shape: {X_stack.shape}\")\n",
    "print(f\"Test stacking features shape: {X_test_stack.shape}\")\n",
    "print(f\"Models in stack: {model_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN META-LEARNER\n",
    "# ============================================================================\n",
    "print(\"\\nüéØ Training meta-learner (Logistic Regression)...\")\n",
    "\n",
    "# Scale stacking features\n",
    "stack_scaler = StandardScaler()\n",
    "X_stack_scaled = stack_scaler.fit_transform(X_stack)\n",
    "X_test_stack_scaled = stack_scaler.transform(X_test_stack)\n",
    "\n",
    "# Train meta-learner with CV\n",
    "meta_oof = np.zeros(len(X_stack))\n",
    "meta_test = np.zeros(len(X_test_stack))\n",
    "meta_scores = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(FOLD_INDICES):\n",
    "    X_tr, X_val = X_stack_scaled[train_idx], X_stack_scaled[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Meta-learner: Logistic Regression\n",
    "    meta_model = LogisticRegression(C=1.0, max_iter=1000, random_state=Config.SEED)\n",
    "    meta_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    val_pred = meta_model.predict_proba(X_val)[:, 1]\n",
    "    test_pred = meta_model.predict_proba(X_test_stack_scaled)[:, 1]\n",
    "    \n",
    "    meta_oof[val_idx] = val_pred\n",
    "    meta_test += test_pred / len(FOLD_INDICES)\n",
    "    \n",
    "    fold_auc = roc_auc_score(y_val, val_pred)\n",
    "    meta_scores.append(fold_auc)\n",
    "    print(f\"   Fold {fold_idx + 1}: AUC = {fold_auc:.5f}\")\n",
    "\n",
    "stacking_auc = roc_auc_score(y, meta_oof)\n",
    "print(f\"\\n‚úÖ Stacking OOF AUC: {stacking_auc:.5f}\")\n",
    "print(f\"   CV Mean: {np.mean(meta_scores):.5f} (+/- {np.std(meta_scores):.5f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÄ Final Blending & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL BLENDING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL BLENDING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best individual models\n",
    "best_models = results_df.head(3)['Model'].tolist()\n",
    "print(f\"\\nTop 3 individual models: {best_models}\")\n",
    "\n",
    "# Blend stacking with simple average of all models\n",
    "simple_avg_oof = np.mean([oof_predictions[name] for name in model_names], axis=0)\n",
    "simple_avg_test = np.mean([test_predictions[name] for name in model_names], axis=0)\n",
    "simple_avg_auc = roc_auc_score(y, simple_avg_oof)\n",
    "print(f\"\\nSimple average OOF AUC: {simple_avg_auc:.5f}\")\n",
    "\n",
    "# Weighted blend: stacking + simple average\n",
    "# Weights can be tuned based on validation performance\n",
    "blend_weights = {\n",
    "    'stacking': 0.5,\n",
    "    'simple_avg': 0.3,\n",
    "    'best_model': 0.2\n",
    "}\n",
    "\n",
    "best_model_name = best_models[0]\n",
    "best_model_oof = oof_predictions[best_model_name]\n",
    "best_model_test = test_predictions[best_model_name]\n",
    "\n",
    "final_oof = (\n",
    "    blend_weights['stacking'] * meta_oof +\n",
    "    blend_weights['simple_avg'] * simple_avg_oof +\n",
    "    blend_weights['best_model'] * best_model_oof\n",
    ")\n",
    "\n",
    "final_test = (\n",
    "    blend_weights['stacking'] * meta_test +\n",
    "    blend_weights['simple_avg'] * simple_avg_test +\n",
    "    blend_weights['best_model'] * best_model_test\n",
    ")\n",
    "\n",
    "final_auc = roc_auc_score(y, final_oof)\n",
    "print(f\"\\nüèÜ FINAL BLEND OOF AUC: {final_auc:.5f}\")\n",
    "print(f\"   Blend weights: {blend_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIMIZE BLEND WEIGHTS (Grid Search)\n",
    "# ============================================================================\n",
    "print(\"\\nüîß Optimizing blend weights...\")\n",
    "\n",
    "best_auc = 0\n",
    "best_weights = None\n",
    "\n",
    "# Grid search over weights\n",
    "for w_stack in np.arange(0.3, 0.8, 0.1):\n",
    "    for w_avg in np.arange(0.1, 0.5, 0.1):\n",
    "        w_best = 1 - w_stack - w_avg\n",
    "        if w_best < 0 or w_best > 0.5:\n",
    "            continue\n",
    "        \n",
    "        blend_oof = w_stack * meta_oof + w_avg * simple_avg_oof + w_best * best_model_oof\n",
    "        auc = roc_auc_score(y, blend_oof)\n",
    "        \n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_weights = {'stacking': w_stack, 'simple_avg': w_avg, 'best_model': w_best}\n",
    "\n",
    "print(f\"\\n‚úÖ Optimized blend weights: {best_weights}\")\n",
    "print(f\"   Optimized OOF AUC: {best_auc:.5f}\")\n",
    "\n",
    "# Apply optimized weights\n",
    "final_test_optimized = (\n",
    "    best_weights['stacking'] * meta_test +\n",
    "    best_weights['simple_avg'] * simple_avg_test +\n",
    "    best_weights['best_model'] * best_model_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERATE SUBMISSION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_fe[Config.ID_COL].values,\n",
    "    'rainfall': final_test_optimized\n",
    "})\n",
    "\n",
    "# Validate submission\n",
    "print(f\"\\nüìã Submission shape: {submission.shape}\")\n",
    "print(f\"   Expected rows: {len(test)}\")\n",
    "print(f\"   Prediction range: [{submission['rainfall'].min():.4f}, {submission['rainfall'].max():.4f}]\")\n",
    "print(f\"   Mean prediction: {submission['rainfall'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nüìã Submission head:\")\n",
    "display(submission.head(10))\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\n‚úÖ Submission saved to 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compile all results\n",
    "all_results = []\n",
    "\n",
    "# Individual models\n",
    "for name in model_names:\n",
    "    all_results.append({\n",
    "        'Method': name,\n",
    "        'Type': 'Base Model',\n",
    "        'OOF AUC': roc_auc_score(y, oof_predictions[name])\n",
    "    })\n",
    "\n",
    "# Ensemble methods\n",
    "all_results.append({'Method': 'Simple Average', 'Type': 'Ensemble', 'OOF AUC': simple_avg_auc})\n",
    "all_results.append({'Method': 'Stacking (Meta-LR)', 'Type': 'Ensemble', 'OOF AUC': stacking_auc})\n",
    "all_results.append({'Method': 'Final Blend (Optimized)', 'Type': 'Ensemble', 'OOF AUC': best_auc})\n",
    "\n",
    "results_summary = pd.DataFrame(all_results).sort_values('OOF AUC', ascending=False)\n",
    "\n",
    "print(\"\\nüìä All Methods Comparison:\")\n",
    "display(results_summary)\n",
    "\n",
    "print(f\"\\nüéØ IMPROVEMENT SUMMARY:\")\n",
    "baseline_auc = 0.871  # Previous best\n",
    "print(f\"   Previous baseline: {baseline_auc:.4f}\")\n",
    "print(f\"   New best (Final Blend): {best_auc:.5f}\")\n",
    "print(f\"   Improvement: +{(best_auc - baseline_auc)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE (from best model)\n",
    "# ============================================================================\n",
    "print(\"\\nüìä Top 20 Most Important Features (from XGBoost):\")\n",
    "\n",
    "# Train final XGBoost on full data for feature importance\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    learning_rate=0.01,\n",
    "    max_depth=6,\n",
    "    n_estimators=500,\n",
    "    random_state=Config.SEED,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "final_xgb.fit(X, y)\n",
    "\n",
    "# Get feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': final_xgb.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "display(importance_df.head(20))\n",
    "\n",
    "print(\"\\n‚úÖ Note: Lag features and humidity-related features are among the most important!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
